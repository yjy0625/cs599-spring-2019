{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Problem 3: Training and Fine-tuning on Fashion MNIST and MNIST\n",
    "Training neural networks with a huge number of parameters on a small dataset greatly affects the networks' generalization ability, often resulting in overfitting. Therefore, more often in practice, one would fine-tune existing networks that are trained on a larger dataset by continuing training on a smaller dataset. To get familiar with the fine-tuning procedure, in this problem you need to train a model from scratch on a split MNIST dataset and then fine-tune it on the other half of split MNIST dataset. Note that we are training models on this toy dataset because of limited computational resources. In most cases, we train models on ImageNet and fine-tune them on smaller datasets.\n",
    "\n",
    "* <b>Learning Objective:</b> In Problem 2, you implemented a covolutional neural network to perform classification task in TensorFlow. In this part of the assignment, we will show you how to use TensorFlow to fine-tune a trained network on a different task.\n",
    "* <b>Provided Codes:</b> We provide the dataset downloading and preprocessing codes, conv2d(), and fc() functions to build the model performing the fine-tuning task.\n",
    "* <b>TODOs:</b> Train a model from scratch on a split MNIST dataset and then fine-tune it on the other split of MNIST dataset. Both the training loss and the training accuracy need to be shown."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A bit of setups\n",
    "import numpy as np\n",
    "import os.path as osp\n",
    "import os\n",
    "import subprocess\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "from matplotlib.pyplot import imshow\n",
    "import matplotlib.image as mpimg\n",
    "\n",
    "%matplotlib inline\n",
    "%reload_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset download and preprocessing function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_data(download_root='data/', dataset='mnist'):\n",
    "    if dataset == 'mnist':\n",
    "        data_url = 'http://yann.lecun.com/exdb/mnist/'\n",
    "    elif dataset == 'fashion_mnist':\n",
    "        data_url = 'http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/'\n",
    "    else:\n",
    "        raise ValueError('Please specify mnist or fashion_mnist.')\n",
    "\n",
    "    data_dir = osp.join(download_root, dataset)\n",
    "    if osp.exists(data_dir):\n",
    "        print('The dataset was downloaded.')\n",
    "        return\n",
    "    else:\n",
    "        os.mkdir(data_dir)\n",
    "\n",
    "    keys = ['train-images-idx3-ubyte.gz', 't10k-images-idx3-ubyte.gz',\n",
    "            'train-labels-idx1-ubyte.gz', 't10k-labels-idx1-ubyte.gz']\n",
    "\n",
    "    for k in keys:\n",
    "        url = (data_url+k).format(**locals())\n",
    "        target_path = osp.join(data_dir, k)\n",
    "        cmd = ['curl', url, '-o', target_path]\n",
    "        print('Downloading ', k)\n",
    "        subprocess.call(cmd)\n",
    "        cmd = ['gzip', '-d', target_path]\n",
    "        print('Unzip ', k)\n",
    "        subprocess.call(cmd)\n",
    "\n",
    "\n",
    "def load_data(data_dir):\n",
    "    num_train = 60000\n",
    "    num_test = 10000\n",
    "\n",
    "    def load_file(filename, num, shape):\n",
    "        fd = open(osp.join(data_dir, filename))\n",
    "        loaded = np.fromfile(file=fd, dtype=np.uint8)\n",
    "        return loaded[num:].reshape(shape).astype(np.float)\n",
    "\n",
    "    train_image = load_file('train-images-idx3-ubyte', 16, (num_train, 28, 28, 1))\n",
    "    train_label = load_file('train-labels-idx1-ubyte', 8, num_train)\n",
    "    test_image = load_file('t10k-images-idx3-ubyte', 16, (num_test, 28, 28, 1))\n",
    "    test_label = load_file('t10k-labels-idx1-ubyte', 8, num_test)\n",
    "    return train_image, train_label, test_image, test_label\n",
    "\n",
    "\n",
    "def split_mnist(data, labels, train=[1,2,3,4,5,6,7], transfer=[0,8,9]):\n",
    "    train_data = []\n",
    "    train_labels = []\n",
    "    transfer_data = []\n",
    "    transfer_labels = []\n",
    "    for i in range(len(data)):\n",
    "        curr_data = data[i]\n",
    "        curr_label = labels[i]\n",
    "        if curr_label in train:\n",
    "            train_data.append(curr_data)\n",
    "            train_labels.append(curr_label)\n",
    "        elif curr_label in transfer:\n",
    "            transfer_data.append(curr_data)\n",
    "            transfer_labels.append(curr_label) \n",
    "        else:\n",
    "            raise\n",
    "    train_data = np.asarray(train_data)\n",
    "    train_labels = np.asarray(train_labels)\n",
    "    transfer_data = np.asarray(transfer_data)\n",
    "    transfer_labels = np.asarray(transfer_labels)\n",
    "    return train_data, train_labels, transfer_data, transfer_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download MNIST\n",
    "download_data(dataset='mnist')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split the dataset into a training set and a transfer set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, Y_train, X_test, Y_test = load_data('data/mnist')\n",
    "X_train, Y_train, X_transfer, Y_transfer = split_mnist(X_train, Y_train)\n",
    "X_test, Y_test, X_test_transfer, Y_test_transfer = split_mnist(X_test, Y_test)\n",
    "\n",
    "datasets = {}\n",
    "datasets['X_train'], datasets['Y_train'] = X_train, Y_train\n",
    "datasets['X_transfer'], datasets['Y_transfer'] = X_transfer[0:200], Y_transfer[0:200]\n",
    "datasets['X_test'], datasets['Y_test'] = X_test, Y_test\n",
    "datasets['X_test_transfer'], datasets['Y_test_transfer'] = X_test_transfer, Y_test_transfer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Visualizing data distributions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = X_train.shape[1]\n",
    "def make_grid(imgs, n_rows, n_cols):\n",
    "    return imgs.reshape(n_rows, n_cols, res, res).swapaxes(1,2).reshape(n_rows * res, n_cols * res)\n",
    "pretrain_grid = make_grid(X_train[:30], 3, 10)\n",
    "transfer_grid = make_grid(X_transfer[:30], 3, 10)\n",
    "def plot_grid(grid, title):\n",
    "    plt.figure(figsize=(10, 30))\n",
    "    imshow(grid, cmap='gray')\n",
    "    plt.title(title)\n",
    "    plt.axis('off')\n",
    "plot_grid(pretrain_grid, \"Pre-training data distribution\")\n",
    "plot_grid(transfer_grid, \"Finetuning data distribution\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Your TODOs: \n",
    "You are asked to pre-train a neural network on a split of the MNIST dataset and then fine-tune on the other split.  \n",
    "Make sure to read every essential lines of code in the following training block, and complete the implementations in the TODO: block."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow.contrib.slim as slim\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "def conv2d(input, output_shape, k=4, s=2, name='conv2d'):\n",
    "    with tf.variable_scope(name):\n",
    "        return slim.conv2d(input, output_shape, [k, k], stride=s)\n",
    "\n",
    "\n",
    "def fc(input, output_shape, act_fn=tf.nn.relu, name='fc'):\n",
    "    with tf.variable_scope(name):\n",
    "        return slim.fully_connected(input, output_shape, activation_fn=act_fn)\n",
    "\n",
    "\n",
    "def train(batch_size=100, num_epoch=3, learning_rate=1e-5,\n",
    "          num_test=10000, datasets=None):\n",
    "\n",
    "    sess = tf.InteractiveSession()\n",
    "    \n",
    "    # Build the model\n",
    "    X = tf.placeholder(tf.float32, [None, 28, 28, 1])\n",
    "    Y = tf.placeholder(tf.int64, [None])\n",
    "    labels = tf.one_hot(Y, 10)\n",
    "    _ = conv2d(X, 32, name='conv1')\n",
    "    _ = conv2d(_, 64, name='conv2')\n",
    "    _ = conv2d(_, 128, name='conv3')\n",
    "    _ = conv2d(_, 256, name='conv4')\n",
    "    _ = tf.reshape(_, [-1, np.prod(_.get_shape().as_list()[1:])])\n",
    "    _ = fc(_, 256, name='fc1')\n",
    "    logits = fc(_, 10, act_fn=None, name='fc2')\n",
    "\n",
    "    loss = tf.nn.softmax_cross_entropy_with_logits(labels=labels, logits=logits)\n",
    "    loss_op = tf.reduce_mean(loss)\n",
    "\n",
    "    global_step = tf.Variable(0, trainable=False)\n",
    "    lr = tf.placeholder(tf.float32, shape=[])\n",
    "    optimizer = tf.train.AdamOptimizer(lr)\n",
    "    train_op = optimizer.minimize(loss, global_step=global_step)\n",
    "\n",
    "    predict = tf.argmax(logits, 1)\n",
    "    correct = tf.equal(predict, Y)\n",
    "    accuracy_op = tf.reduce_mean(tf.cast(correct, tf.float32))\n",
    "\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "\n",
    "    # variables you will need\n",
    "    total_loss = []\n",
    "    total_accuracy = []\n",
    "    total_loss_scratch = []\n",
    "    total_accuracy_scratch = []\n",
    "    \n",
    "    # evaluation function\n",
    "    def eval(data, labels):\n",
    "        eval_accuracy = 0.0\n",
    "        eval_iter = 0\n",
    "        for i in range(data.shape[0] // batch_size):\n",
    "            X_ = data[i * batch_size:(i + 1) * batch_size][:]\n",
    "            Y_ = labels[i * batch_size:(i + 1) * batch_size]\n",
    "            accuracy = sess.run(accuracy_op, feed_dict = {X: X_, Y: Y_})\n",
    "            eval_accuracy += accuracy\n",
    "            eval_iter += 1\n",
    "        return (eval_accuracy / eval_iter)\n",
    "    \n",
    "    # datasets\n",
    "    X_train, Y_train = datasets['X_train'], datasets['Y_train']\n",
    "    X_transfer, Y_transfer = datasets['X_transfer'], datasets['Y_transfer']\n",
    "    X_test, Y_test = datasets['X_test'], datasets['Y_test']\n",
    "    X_test_transfer, Y_test_transfer = datasets['X_test_transfer'], datasets['Y_test_transfer']\n",
    "    num_train = len(X_train)\n",
    "    num_transfer = len(X_transfer)\n",
    "    print ('Number of training points: {}'.format(num_train))\n",
    "    print ('Number of transferring points: {}'.format(num_transfer))\n",
    "    \n",
    "    # Train the model from scratch on MNIST 0 8 9\n",
    "    print('\\033[93mTraining from scratch on MNIST 0 8 9\\033[0m')\n",
    "    learning_rate = 1e-4\n",
    "    for epoch in range(num_epoch):\n",
    "        for i in range(num_transfer // batch_size):\n",
    "            X_ = X_transfer[i * batch_size:(i + 1) * batch_size][:]\n",
    "            Y_ = Y_transfer[i * batch_size:(i + 1) * batch_size]\n",
    "            [_, loss, accuracy] = sess.run([train_op, loss_op, accuracy_op],\n",
    "                                           feed_dict = {X: X_, Y: Y_, lr: learning_rate})\n",
    "            total_loss_scratch.append(loss)\n",
    "            total_accuracy_scratch.append(accuracy)\n",
    "        print('[Epoch {}] loss: {}, accuracy: {}'.format(epoch, loss, accuracy))\n",
    "    test_acc_wo_transfer = eval(X_test_transfer, Y_test_transfer)\n",
    "\n",
    "    # re-intialize for experimenting our transfer learning\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "\n",
    "    #############################################################################\n",
    "    # TODO: Train the model on MNIST 1 2 3 4 5 6 7 from scratch                 #\n",
    "    # and then fine-tune it on MNIST 0 8 9                                      #\n",
    "    # Collect the training loss and the training accuracy                       #\n",
    "    # fetched from each iteration                                               #\n",
    "    # You should just store the training losses and accuracies during           #\n",
    "    # fine-tuning in order to be compared with training from scratch            #\n",
    "    #                                                                           #\n",
    "    # After the two stages of the training (first pre-train and then            #\n",
    "    # fine-tune), the length of total_loss and total_accuracy should be:        #\n",
    "    # num_epoch * num_transfer / batch_size                                     #\n",
    "    #############################################################################\n",
    "\n",
    "    # Train the model on MNIST 1 2 3 4 5 6 7\n",
    "    print('\\033[93mPre-training on MNIST 1 2 3 4 5 6 7\\033[0m')\n",
    "        \n",
    "    # Fine-tune the model on MNIST 0 8 9\n",
    "    print('\\033[93mFine-tuning on MNIST 0 8 9\\033[0m')\n",
    "    \n",
    "    #############################################################################\n",
    "    #                             END OF YOUR CODE                              #\n",
    "    #############################################################################   \n",
    "    test_acc_w_transfer = eval(X_test_transfer, Y_test_transfer)\n",
    "\n",
    "    return total_loss, total_accuracy,\\\n",
    "           total_loss_scratch, total_accuracy_scratch,\\\n",
    "           test_acc_w_transfer, test_acc_wo_transfer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "loss, accuracy, loss_scratch, accuracy_scratch, test_acc_w_transfer,\\\n",
    "    test_acc_wo_transfer = train(datasets=datasets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check your results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print ('\\033[93mTesting Accuracy w/o transfer learning: {}\\033[0m'.format(test_acc_wo_transfer))\n",
    "print ('\\033[93mTesting Accuracy w/ transfer learning:  {}\\033[0m'.format(test_acc_w_transfer))\n",
    "\n",
    "# Plot the training loss and the training accuracy\n",
    "N = 50\n",
    "plt.plot(loss_scratch[0:N], label='w/o transfer')\n",
    "plt.plot(loss[0:N], label='w/ transfer')\n",
    "plt.legend(loc='upper right')\n",
    "plt.title('training loss')\n",
    "plt.xlabel('iteration')\n",
    "plt.ylabel('loss')\n",
    "plt.show()    \n",
    "\n",
    "plt.plot(accuracy_scratch[0:N], label='w/o transfer')\n",
    "plt.plot(accuracy[0:N], label='w/ transfer')\n",
    "plt.legend(loc='lower right')\n",
    "plt.title('training accuracy')\n",
    "plt.xlabel('iteration')\n",
    "plt.ylabel('accuracy')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inline Question: Describe what you observe from the above test set results and the plots\n",
    "#### Ans:"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
